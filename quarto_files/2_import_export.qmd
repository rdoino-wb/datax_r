---
title: "Data Importing, Tyding and Writing"
subtitle: "R Training"
institute: "The World Bank Group - DataTax"
date: today
author: ""
date-format: "dddd, [the] D[<sup style='font-size:65%;font-style:italic;'>th</sup>] [of] MMMM, YYYY"
knitr:
  opts_chunk:
    dev: 'png'
format:
  dime-revealjs:
    embed-resources: true
    # Output file
    output-file: 2_import_export.html
    code-line-numbers: true
    # Table of Contents
    toc: false
    toc_float: false
    toc-depth: 1
    toc-location: left
    toc-title: "Agenda"
    # Generate a self contained file
    self-contained: true
    self-contained-math: true
    # Turn preview links within the presentation off (all links open in a new tab)
    preview-links: true
    auto-animate: true
    # Logo and footer options
    logo: "logos/WB-DEC-Impact-horizontal-RGB-high.png"
    footer: "Code available on [GitHub](https://github.com/rdoino-wb/data_analysis_DataTable)."
---

```{r set-up}

options(repos = c(CRAN = "https://cloud.r-project.org"))

library(emo)
library(countdown)

# Load the readxl package
library(readxl)
library(dplyr)

# Importing data from an .xlsx file
data <- read_excel("Exercises/data_tax.xlsx")

```


------------------------------------------------------------------------

## Good Practice in Project Organization

- When starting a new analysis, organize your work by creating a structured folder system:

- `r emo::ji("folder")` `r_training`
  - `r emo::ji("folder")` `scripts/` (code)
  - `r emo::ji("folder")` `data/` (datasets)
  - `r emo::ji("folder")` `outputs/` (results like plots, tables, etc.)

```{mermaid}
%%| fig-align: center
%%| fig-width: 1200
%%| fig-height: 800

graph TD;
    A[r_training] --> B[scripts];
    A --> C[data];
    A --> D[outputs];
```    

:::{.callout-note}
Use lowercase and hyphens (`-`) instead of spaces when naming folders, files, and objects in R to maintain consistency and ease management.
:::

----

## Understanding Folder Paths

- A path is an address that tells a software where to find a file or folder on your computer.

**Two Types of Paths:**

1. **Absolute Path**: `/Users/yourname/Desktop/r_training`
2. **Relative Path**: `r_training/scripts`

![](images/paths.png){fig-align="center" width="70%"}

------------------------------------------------------------------------

## The Advantage of Using Projects

`R` doesn’t automatically know where your files are. Using an RStudio project creates a shortcut that tells `R` where to find everything, making your workflow smoother.

![](images/project.png){width="auto" fig-align="center"}

------------------------------------------------------------------------

## Setting Up Your Environment {.exercise}

`r countdown::countdown(minutes = 5, seconds = 0, top = 0)`

1. **Create a new Folder named `r_training`**

2. **Create a Project**  
   - Open **RStudio**.  
   - Go to **File > New Project > Existing Directory**.  
   - Browse to your `r_training` folder and click **Open**.  
   - Click **Create Project** to finish.

3. **Open the Project**  (Double-click the file to open it in RStudio.)

4. **Run this command in the RStudio Console:  ** 

```{r usecourse, echo = T, eval = F}
usethis::use_course("https://github.com/your-organization/course-repo")
```
     
   - Follow the prompts to unzip the materials into your project folder.

You’re all set to begin! `r emo::ji("party_popper")`

------------------------------------------------------------------------

## Organizing the `data` Folder

```{mermaid}
%%| fig-align: center

graph TD;
    A[data] --> B[Raw];
    A --> C[Intermediate];
    A --> D[Final];
```

:::: {.columns}

::: {.column width="33%" .fragment}
### `Raw`
- Original, untouched data.  
- Backup recommended to preserve integrity.  
:::

::: {.column width="33%" .fragment}
### `Intermediate`
- Data formatted, renamed, and tidied.  
- Ready for further cleaning.  
:::

::: {.column width="33%" .fragment}
### `Final`
- Cleaned and transformed data.  
- Ready for graphs, tables, and regressions.  
:::

::::

# Import

## Preparing Data for R: General Concepts and Best Practices

::: {.incremental}

- **Document Types** organizations extensively rely on spreadsheets ([read](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989#abstract))
  - Common data formats include:
    - **Spreadsheets** (.csv, .xlsx, xls...): Standard for structured data.
    - **DTA** (.dta): Used for data from STATA.

- **Spreadsheets**  
  - **CSV** is generally preferable:
    - Easier to import and process.
    - More compatible across different systems and software and **much lighter**.

:::

. . . 

::: {.callout-caution collapse="true"}
# Advanced 
The [Apache Arrow](https://arrow.apache.org/) format is designed to handle large data sets efficiently, making it suitable for big data analysis. Arrow files offer faster read/write operations compared to traditional formats.
:::

---

## Importing Data into R

- **Main Import Methods**  
  - The library [readxl](https://readxl.tidyverse.org/) provides some useful functions
    - **read_xls()** for XLS files (*.xls*).
    - **read_xlsx()** for Excel files (*.xlsx*).
    - **read_csv()** CSV files (*.csv*).
    - **read_parquet()** for *.dta* files, using the [haven](https://haven.tidyverse.org/) package.
    - **readRDS()** for *.RDS* files.

. . . 

- **Example: Importing an .xlsx File**  
  - To import an Excel file, you’ll need to load the `readxl` package

```{r import, echo = T, eval = F}
# Install the package if necessary
install.packages("readxl")

# Load the readxl package
library(readxl)

# Importing data from an .xlsx file
data <- read_excel("data_tax.xlsx")
```

---

## Practical Steps Right After Importing Data

1. Once data is imported, we first want to take a look at it `r emo::ji("eyes")`
  
```{r look, echo = T, eval = T}

# print the first 5 rows
head(data)

# open the entire dataset 
# View(data)

# we can mix them together
# View(head(data))

```

. . . 

::: {.callout-note}
You might also notice that the Taxpayer ID and Full Name columns are surrounded by back-ticks. This is because they contain spaces, which breaks R’s standard naming rules, making them non-syntactic names. To refer to these variables in R, you need to enclose them in back-ticks.
:::

---

## Practical Steps Right After Importing Data

2. Let's get a snapshot of the data structure and content: 

```{r snapshot, echo = T, eval = T}

# Install the package if necessary
# install.packages("dplyr")

# Load the dplyr package
library(dplyr)

# Get an overview of the data
glimpse(data)

```
---

## Practical Steps Right After Importing Data

3.	Now, we’ll make sure that our variable names follow snake_case convention `r emo::ji("sunglasses")`

- Option 1: Rename columns manually:
  
```{r rename1, echo = T, eval = F}

# We can rename columns one by one
data = rename(
  data, 
  tax_payer_id    = `Taxpayer ID`,
  name            = `Name`
  ...
  )

```

. . . 

-	Option 2: Automatically convert all column names to snake_case using janitor:
	
```{r rename2, echo = T, eval = T}

# Install the package if necessary
# install.packages("janitor")

# Load the janitor package
library(janitor)

# transform in snake_case all the var names
data = clean_names(data)

# take a look
names(data)

```

---

## Exercise 1 {.exercise}

You can find the exercise in the folder "Exercises/Import_and_Export.R"

`r countdown::countdown(minutes = 10, seconds = 0, top = 10)`

```{r exercise1, echo = T, eval = F}

# Exercise: Data Import and Initial Cleaning ----

# Step 1: Setting Up

# List of required packages
packages = c(
  "readxl",      # For reading Excel files
  "dplyr",       # For data manipulation
  "tidyverse",   # For general data handling and visualization
  "data.table",  # For fast data import and manipulation
  "here",        # For file path management
  "haven",       # For importing Stata files
  "janitor"      # For cleaning column names
)

# Install pacman if not already installed
if (!require("pacman")) install.packages("pacman")

# Load the packages with pacman
pacman::p_load(packages, character.only = TRUE, install = TRUE)

# Step 2: Loading and Inspecting Data ----

# 1. Firm Characteristics Data set
# Hint: Use an appropriate function to load a CSV file.
# Objective: Load "firm_characteristics.csv" and check the first few rows.

# Hint: After loading, apply a function from `janitor` to clean column names.

# 2. VAT Declarations Data set
# Hint: Use a function that can read Stata (.dta) files.
# Objective: Load "vat_declarations.dta" and check the first few rows.

# Hint: Review the column names to see if they need cleaning.

# 3. CIT Declarations Data set
# Hint: Use a function that can read Excel files and specify the sheet.
# Objective: Load "cit_declarations.xlsx" from the second sheet and check the first few rows.

# Reflection (BONUS) 
# After completing each step, ensure column names are consistent across files.

```
---

## Exercise 1 (Solutions) {.exercise}

```{r solutions1, echo = T, eval = F}

# Pre-requisites ----

# List of required packages
packages = c(
  "readxl",
  "dplyr",
  "tidyverse",
  "data.table",
  "here",
  "haven",
  "janitor" # Added janitor for clean_names() function
)

# Install pacman if not already installed
if (!require("pacman")) install.packages("pacman")

# Load the packages with pacman
pacman::p_load(packages, character.only = TRUE, install = TRUE)

# Import the Data ----

# FIRMS CHARACTERISTICS

# Load the firm-level characteristics (firm_characteristics.csv)
dt_firms = fread(here("quarto_files", "Solutions", "Data", "Raw", "firm_characteristics.csv"))

# Display the first 5 rows
head(dt_firms)

# Clean column names to make them consistent
dt_firms = clean_names(dt_firms)

# VAT DECLARATIONS

# Load the VAT panel data (vat_declarations.dta)
panel_vat = read_dta(here("quarto_files", "Solutions", "Data", "Raw", "vat_declarations.dta"))

# Display the first 5 rows
head(panel_vat)

# Column names look fine, but the bonus question requires us to change firm_id
panel_vat = rename(panel_vat, firm_id = id_firm)

# CIT DECLARATIONS

# Load the CIT panel data (cit_declarations.xlsx)
panel_cit = read_xlsx(here("quarto_files", "Solutions", "Data", "Raw", "cit_declarations.xlsx"), sheet = 2)

# Display the first 5 rows
head(panel_cit)

# Column names look consistent, so no need for further cleaning

```

# Tyding Data 

## Tidy Tax Administrative Data {.center}

. . . 

**What is Tidy Data?**

Tidy data organizes tax administrative data into a consistent, analysis-ready format:

. . . 

1. **Each variable is a column; each column is a variable.**

. . .

2. **Each observation is a row; each row is an observation.**

. . .

3. **Each value is a cell; each cell is a single value.**

. . .

![](images/tidy.png){fig-align="center"}


## Let's take a look at this dataset 

::: {.fragment}

| Taxpayer ID | Tax Type   | 2021 Q1 | 2021 Q2 | 2021 Q3 | 2021 Q4 |
|-------------|------------|---------|---------|---------|---------|
| 101         | Income Tax | 500     | 600     | 450     | 700     |
| 102         | VAT        | 300     | 400     | 350     | 500     |

:::

::: {.fragment}

**What are the variables?** 

:::

::: {.fragment}

Taxpayer ID, Tax Type, Quarter, Payment Amount.

::: 

::: {.fragment}

**What constitutes a single observation in this dataset?** 

:::

::: {.fragment}

One observation is a specific tax payment for a taxpayer during a particular quarter.

::: 

::: {.fragment}

**How would you reshape the dataset to meet the three key tidy characteristics?** 

  - Each variable is a column; each column is a variable.

  - Each observation is a row; each row is an observation.

  - Each value is a cell; each cell is a single value.

:::

---

**From "Wide"**

| Taxpayer ID | Tax Type   | 2021 Q1 | 2021 Q2 | 2021 Q3 | 2021 Q4 |
|-------------|------------|---------|---------|---------|---------|
| 101         | Income Tax | 500     | 600     | 450     | 700     |
| 102         | VAT        | 300     | 400     | 350     | 500     |

. . . 

**To "Long" --> Tidy Data**

| Taxpayer ID | Tax Type   | Quarter | Payment Amount |
|-------------|------------|---------|----------------|
| 101         | Income Tax | 2021 Q1 | 500            |
| 101         | Income Tax | 2021 Q2 | 600            |
| 101         | Income Tax | 2021 Q3 | 450            |
| 101         | Income Tax | 2021 Q4 | 700            |
| 102         | VAT        | 2021 Q1 | 300            |
| 102         | VAT        | 2021 Q2 | 400            |
| 102         | VAT        | 2021 Q3 | 350            |
| 102         | VAT        | 2021 Q4 | 500            |

---

## How Do We Go from “Wide” to “Long” in R?

. . . 


:::: {.columns}

::: {.column width=20%}
![](images/tidyr.png){fig-align="center" width=75px fig-height=auto}
:::

::: {.column width=80%}
<div style="margin-top: 35px;">
We need to introduce the package [tidyr](https://tidyr.tidyverse.org/).  
</div>
:::

::::

. . . 

**Why Use Packages?**

-	Packages contain tools (or “verbs”) to perform specific tasks in R.
-	Once installed and loaded, these verbs simplify complex operations.
	
``` {r tidyr, echo = T, eval = T}
install.packages("tidyr") #  the package
library(tidyr) # load it
```

```{r, echo = F, eval = T}
install.packages("tibble")
library(tibble)

# Create a wide dataset
wide_data <- tribble(
  ~`taxpayer_id`, ~`tax_type`, ~`2021 Q1`, ~`2021 Q2`, ~`2021 Q3`, ~`2021 Q4`,
  101, "Income Tax", 500, 600, 450, 700,
  102, "VAT", 300, 400, 350, 500,
  102, "Income Tax", 530, 110, 350, 400,
  101, "VAT", 100, 700, 850, 400
)
```

:::{.callout-note}
Use cheat sheets for quick summaries of a package’s verbs and their usage (e.g., [tidyr Cheat Sheet](https://github.com/rstudio/cheatsheets/blob/main/tidyr.pdf)). And don’t forget! AI tools can also assist with coding and debugging. 
:::

. . . 

---

## In practice: 

Start with a `wide` dataset

``` {r wide, echo = F, eval = T}

print(wide_data)
```

. . . 

```{r, echo = T, eval = T}
#| collapse: true
#| code-line-numbers: "1,2|4,5|7,8,9,10,11,12,13,14,15,16,17|7,8,9|10,11,12,13,14|15|16|"
# Install the package 
install.packages("tidyr")

# Load the package in our session
library(tidyr)

# Pivot the dataset to long format
long_tax_data <- pivot_longer(
    wide_data,                  # Dataset name
    cols = c(
      "2021 Q1",
      "2021 Q2", 
      "2021 Q3", 
      "2021 Q4"),               # Select columns to pivot
    names_to = "Quarter",       # New column for the old column headers (quarters)
    values_to = "Amount"        # New column for the values
  )

```

---

## The final result

```{r, echo = T, eval = T}

# Print the final result
print(long_tax_data)

```

---

## Let's start with two key definitions

:::: {.columns}

::: {.column width="50%" .fragment}

::: {.callout-note icon=false appearence=simple}
## Unit of Observation
The unit of observation refers to the smallest entity or item for which data is collected in a dataset.
:::

:::

::: {.column width="50%" .fragment}

::: {.callout-note icon=false appearence=simple}
## Unit of Analysis
The unit of analysis refers to the entity or object about which you are trying to make conclusions or draw insights.
:::

:::

::::

:::: {.columns}

::: {.column width="50%" .fragment}

::: {.callout-note icon=false appearence=simple}
## Unit of Observation
The unit of observation refers to the smallest entity or item for which data is collected in a dataset.
:::

:::

::: {.column width="50%" .fragment}

::: {.callout-note icon=false appearence=simple}
## Unit of Analysis
The unit of analysis refers to the entity or object about which you are trying to make conclusions or draw insights.
:::

:::

::::

--- 

## Unit of Analysis

1. We need to identify the Unit of Analysis:
  - If studying the different sources of taxes --> `Transaction` X `Tax` X `Time` 
  - If looking at taxpaers behavior --> `Tax ID` X `Time`
  
2. The Unit of Analysis always needs to be uniquely identified by an ID (combined with time periods in the case of a panel).

. . . 

3. How do we now move from `Long` to `Wide`? 

```{r, echo = F, eval = T}

wide_tax_data <- long_tax_data %>%
  pivot_wider(
    names_from = `tax_type`,       # Use `Tax Type` to create new column names
    values_from = Amount          # Populate the new columns with `Amount` values
  )

# Print the final result
print(wide_tax_data)

```

---

# Writing Data in R

## Writing in .csv Format is (Almost) Always a Good Choice

- For most cases, writing data in .csv format is a reliable and widely compatible option.

- I recommend using the `fwrite` function from the `data.table` package for its speed and efficiency.

. . .  

- Below, we save various datasets into the Intermediate folder using fwrite:

```{r write, echo = T, eval = F}

# Write the VAT Data
fwrite(panel_vat, here("quarto_files", "Solutions", "Data", "Intermediate", "panel_vat.csv"))

# Write the CIT Declarations
fwrite(panel_cit, here("quarto_files", "Solutions", "Data", "Intermediate", "panel_cit.csv"))
       
# Write the Firm Characteristics
fwrite(dt_firms, here("quarto_files", "Solutions", "Data", "Intermediate", "dt_firms.csv"))
       
```    

## There other options to write data

- Writing .rds Files (For R Objects)

	-	The .rds format is specifically designed for saving R objects. It is useful for saving intermediate results, objects, or data.
	-	We’ll explore this format in more detail later, but here’s a quick example:
  
```{r write_rds, echo = T, eval = F}

# Example
base::saveRDS(data, "data_tax.rds")
       
```    

. . .

  - Writing .xlsx Files (For Excel Compatibility): To save data in Excel format (.xlsx), use the writexl package. It is lightweight and doesn’t require external dependencies.

```{r write_xlsx, echo = T, eval = F}

# Example
writexl::write_xlsx(data, "data_tax.xlsx")
       
```  
  
. . . 

  - Writing .parquet Files (For Large Datasets): The .parquet format is a columnar storage format that is highly efficient for both reading and writing large datasets (typically >1GB).

```{r write_parquet, echo = T, eval = F}

# Example
arrow::writea_parquet(data, "data_tax.parquet")

# Reading parquet files
arrow::read_parquet("data_tax.parquet")
       
```  

---

## To Sum Up

![](images/write_import.png){fig-align="center"}

---

## Exercise 2 

You can find the exercise in the folder "Exercises/Import_and_Export.R"

`r countdown::countdown(minutes = 5, seconds = 0, top = 10)`

```{r exercise2, echo = T, eval = F}

# Exercise 2: Write the Data ----

# FIRMS CHARACTERISTICS

# Write the firm-level characteristics (firm_characteristics.format)
# Don't forget to put in the right folder and pick the best format


# VAT DECLARATIONS

# Write the VAT panel data (vat_declarations.format)


# CIT DECLARATIONS

# Write the CIT panel data (cit_declarations.format)

```
---

## Exercise 2 (Solutions)

```{r solutions2, echo = T, eval = F}

# Exercise 2: Write the Data ----

# FIRMS CHARACTERISTICS
fwrite(dt_firms, here("quarto_files", "Solutions", "Data", "Intermediate", "dt_firms.csv")

# VAT DECLARATIONS

# Write the VAT panel data (vat_declarations.format)
fwrite(panel_vat, here("quarto_files", "Solutions", "Data", "Intermediate", "panel_vat.csv"))

# CIT DECLARATIONS

# Write the CIT panel data 
fwrite(panel_cit, here("quarto_files", "Solutions", "Data", "Intermediate", "panel_cit.csv"))

```

# Bonus: Connecting `R` to Databases

---

::: {.incremental}

- **Why Connect to Databases?**  
  - Data is often stored in centralized databases for enhanced security, accessibility, and management.  
  - Traditional workflows might involve submitting data requests to IT teams, causing delays and limited flexibility for analysts.

---

- **The Power of `R` for Database Access**  
  - Using `R`, you can:
    - Query data directly and in real-time.
    - Import large datasets seamlessly into your `R` environment.xw

---

- **The `DBI` Package: A Common Interface** supporting a lot of dabases: **MySQL**, **PostgreSQL**, **SQLite**...

:::

---

## Example: connecting to a database

```{r database, echo = T, eval = F}
#| code-line-numbers: "1,2,3|5,6,7,8,9,10,11|13,14,15|17,18|"

# Load Packages
library(DBI) # this package is always needed
library(RMariaDB) # there are packages for each type of database (e.g. RPostgreSQL)

# Example connection
con <- dbConnect(RMariaDB::MariaDB(),
  dbname = "your_db_name",
  host = "your_host",
  user = "your_user",
  password = "your_password"
)

# Query data
result <- dbGetQuery(con, "SELECT * FROM your_table LIMIT 5")
print(result)

# Disconnect
dbDisconnect(con)

```

---

## Writing Dynamic SQL Queries with `glue_sql`

```{r database_glue, echo = T, eval = T}
#| collapse: true
#| code-line-numbers: "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24|26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41|"

# Load required packages
library(DBI)
library(RSQLite)
library(glue)

# Connect to SQLite database
con <- dbConnect(RSQLite::SQLite(), ":memory:")

# Create a Mock DB
tax_data <- data.frame(
  taxpayer_id = paste0("TP", sprintf("%03d", 1:10)),
  name = c("Aisha Khan", "Carlos Martinez", "Chen Wei", "Elena Petrova", "Fatima Al-Farsi",
           "George Okoro", "Hiroshi Tanaka", "Leila Kaur", "Maria Rossi", "William Smith"),
  income = c(85000, 62000, 78000, 90000, 68000, 72000, 95000, 56000, 89000, 75000),
  tax_paid = c(8500, 6200, 7800, 9000, 6800, 7200, 9500, 5600, 8900, 7500),
  tax_year = rep(2022, 10)
)

# Write the dataset to the database using dbWriteTable
dbWriteTable(con, "tax_records", tax_data, overwrite = TRUE)

# Use glue_sql for a dynamic query
min_income <- 60000
query <- glue_sql("SELECT * FROM tax_records WHERE income > {min_income}", .con = con)

# Execute the query
result <- dbGetQuery(con, query)
print(result)

# Disconnect
dbDisconnect(con)

```

---

## Further Resources for SQL

- Take a look at this extensive [list of free resources](https://github.com/amartinson193/The-Ultimate-List-of-Free-SQL-Resources)
  
- Great Graduate-level [course](https://raw.githack.com/uo-ec607/lectures/master/16-databases/16-databases.html#Requirements) taught by [Grant R. McDermott](https://grantmcdermott.com/)

- RStudio’s [Databases using R](https://solutions.posit.co/connections/db/) is an excellent resource that covers a lot. 

- [Juan Mayorga](http://jsmayorga.com/)’s tutorial, [Getting Global Fishing Watch Data from Google Big Query using R](http://jsmayorga.com/post/getting-global-fishing-watch-from-google-bigquery-using-r/), offers a hands-on guide to accessing data and highlights why learning SQL is valuable beyond just using the `dplyr` translation.

- For a concise introduction to SQL, Julia Evans’ [Become A Select Star](https://wizardzines.com/zines/sql/) is a highly recommended resource.

- Google’s official BigQuery [documentation](https://cloud.google.com/bigquery/docs/) provides a deep dive into SQL functions, syntax, and specialized operations, such as handling datetime and JSON objects.

- Platforms like [W3Schools](https://www.w3schools.com/sql/default.asp) and [Codecademy](https://www.codecademy.com/learn/learn-sql) offer numerous free and paid SQL tutorials and courses to enhance your skills.